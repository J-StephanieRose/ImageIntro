{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "try:\n",
    "    from tqdm import tqdm_notebook\n",
    "except:\n",
    "    def tqdm_notebook(iterable):\n",
    "        return iterable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome!\n",
    "\n",
    "Before we begin, make sure you have the following files downloaded and unzipped in this directory\n",
    "\n",
    "[MSCOCO data](https://drive.google.com/file/d/1hJrp-vn44zKPNknmMkvlPvRf2qrErzmB/view?usp=sharing)\n",
    "[British Library data](https://drive.google.com/file/d/1bJ-l9HchOzLXIhCGecz0sS3arPhZoA1H/view?usp=sharing)\n",
    "\n",
    "you can run the following cell to check to make sure that you have this done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists('coco_workshop/0.jpg'):\n",
    "    print(\"Please download the COCO workshop data.\")\n",
    "if not os.path.exists('bl_workshop/0.jpg'):\n",
    "    print(\"Please download the BL workshop data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets see what files are in these folders!\n",
    "files = os.listdir('coco_workshop')\n",
    "print(files[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Loading images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the workshop, we will show how to load an image into python using the python image library (PIL). We will also display the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "file_path = 'coco_workshop/22.jpg'\n",
    "my_image = Image.open(file_path)\n",
    "display(my_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(my_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## methods can be called on my_image using my_image.method_name([arguments, if they are there])\n",
    "\n",
    "# can you print out the dimensions of the image? I.e., it's width and height?\n",
    "\n",
    "#print(my_image.XXXX())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Image Filters: Not Just for Instagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets try a few fun filters out on the image!\n",
    "from PIL import ImageFilter\n",
    "\n",
    "modified_image = my_image.filter(ImageFilter.BLUR)\n",
    "display(modified_image)\n",
    "modified_image = my_image.filter(ImageFilter.EDGE_ENHANCE)\n",
    "display(modified_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(ImageFilter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# try out a few more filters, e.g., edge detection\n",
    "# modified_image = my_image.filter([put filter in here])\n",
    "# display(modified_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Images as Arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To a computer, an image is simply a big list of numbers. In particular, a useful representation of the image is the Red-Green-Blue (RGB) space representation. In the same way a printer can use red, green, and blue ink to represent most colors, a computer can, too. For each \"pixel\" in the input image, there will be three values that express to the computer what color is should be: a red value, a green value, and a blue value. So -- an image can be represented by a long list of numbers: for each x coordinate and for each y coordinate there will be a R,G,B triple. [Image source](https://web.stanford.edu/class/cs101/image-1-introduction.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![How a computer represents an image](images/array.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "image_array = np.asarray(my_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do the dimensions of this array represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import isolate_channel\n",
    "red_intensity_map = isolate_channel(image_array, 0)\n",
    "blue_intensity_map = isolate_channel(image_array, 1)\n",
    "green_intensity_map = isolate_channel(image_array, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_map = Image.fromarray(red_intensity_map)\n",
    "green_map = Image.fromarray(green_intensity_map)\n",
    "blue_map = Image.fromarray(blue_intensity_map)\n",
    "display(my_image)\n",
    "display(red_map)\n",
    "display(green_map)\n",
    "display(blue_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do these images really combine to form the original?\n",
    "combined_map = red_intensity_map + green_intensity_map + blue_intensity_map\n",
    "combined_image = Image.fromarray(combined_map)\n",
    "display(combined_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using a few calls to my_image.putpixel((x,y), (r,g,b)) can you draw a blue box in the image?\n",
    "\n",
    "for idx1 in range(90,110):\n",
    "    for idx2 in range(90,110):\n",
    "        pass\n",
    "        # my_image.putpixel((?,?),(?,?,?))\n",
    "display(my_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: 2d-convolutions as filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Briefly returning to filters: have you ever thought about how a computer might compute a \"blur\"? Or an \"edge detection\"? The core of these implementations is the convolution! Let's talk about that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'coco_workshop/22.jpg'\n",
    "my_image = Image.open(file_path).convert('L')\n",
    "display(my_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for convolution, it helps to normalize the exposure first so the effects can be more readily seen\n",
    "from convolution import equalize_exposure\n",
    "my_image_equalized = equalize_exposure(my_image)\n",
    "display(my_image_equalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blur_3by3 = np.array([[1,1,1],\n",
    "                      [1,1,1],\n",
    "                      [1,1,1]])\n",
    "\n",
    "blur_5by5 = np.array([[1,1,1,1,1],\n",
    "                      [1,1,1,1,1],\n",
    "                      [1,1,1,1,1],\n",
    "                      [1,1,1,1,1],\n",
    "                      [1,1,1,1,1]])\n",
    "\n",
    "blur_7by7 = np.array([[1,1,1,1,1,1,1],\n",
    "                      [1,1,1,1,1,1,1],\n",
    "                      [1,1,1,1,1,1,1],\n",
    "                      [1,1,1,1,1,1,1],\n",
    "                      [1,1,1,1,1,1,1],\n",
    "                      [1,1,1,1,1,1,1],\n",
    "                      [1,1,1,1,1,1,1]])\n",
    "\n",
    "edge_detect = np.array([[-1,-1,-1],\n",
    "                        [-1,8,-1],\n",
    "                        [-1,-1,-1]])\n",
    "\n",
    "sharpen = np.array([[0,1,0],\n",
    "                    [1,4,1],\n",
    "                    [0,1,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from convolution import convolve\n",
    "\n",
    "res = convolve(my_image, edge_detect)\n",
    "display(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from convolution import convolve\n",
    "my_filter = np.array([[0,0,0],\n",
    "                      [0,4,0],\n",
    "                      [0,0,0]])\n",
    "res = convolve(my_image, my_filter)\n",
    "display(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part 5: Representing images as their mean color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One goal of computer vision is to come up with vector representations of images. Lets see if we can use the average color for an image as a representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_path1 = 'coco_workshop/22.jpg'\n",
    "image_path2 = 'coco_workshop/23.jpg'\n",
    "image_path3 = 'coco_workshop/24.jpg'\n",
    "image1 = Image.open(image_path1)\n",
    "image2 = Image.open(image_path2)\n",
    "image3 = Image.open(image_path3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import compute_mean_color\n",
    "mean_red, mean_green, mean_blue = compute_mean_color(image1)\n",
    "display(image1)\n",
    "print(mean_red, mean_green, mean_blue)\n",
    "mean_red, mean_green, mean_blue = compute_mean_color(image2)\n",
    "display(image2)\n",
    "print(mean_red, mean_green, mean_blue)\n",
    "mean_red, mean_green, mean_blue = compute_mean_color(image3)\n",
    "display(image3)\n",
    "print(mean_red, mean_green, mean_blue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do these numbers mean about images 1/2/3? What can you tell about images based on their mean color?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from util import constant_color_image\n",
    "display(constant_color_image(mean_red, mean_green, mean_blue))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets display these three images and their mean colors..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in [image_path1, image_path2, image_path3]:\n",
    "    print(\"Image: \" + path)\n",
    "    image = Image.open(path)\n",
    "    display(image)\n",
    "    # step 1: compute the mean color of the current image.\n",
    "    # step 2: make a constant_color_image for this mean_red, mean_gree, mean_blue image\n",
    "    # step 3: display the constant_color_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets get the mean color for all images!\n",
    "base_path = 'coco_workshop/'\n",
    "n_images = 5000\n",
    "all_means = np.empty((n_images, 3)) #indexed by image_idx, channel\n",
    "for idx in range(n_images):\n",
    "    fname = base_path + str(idx) + \".jpg\"\n",
    "    print(\"processing \" + fname)\n",
    "    image = Image.open(path)\n",
    "    mean_red, mean_green, mean_blue = compute_mean_color(image)\n",
    "    all_means[idx,0] = mean_red\n",
    "    all_means[idx,1] = mean_green\n",
    "    all_means[idx,2] = mean_blue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(all_means[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(all_means.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Nearest neighbor search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a vector representation of each image (specifically: our vector is of length 3, representing the average color) we can search for nearest neighbors of each image according to this representation. In this space, two images are similar if they have similar average colors. Is this a good notion of similarity? What could this potentially be used for?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the [Annoy library](https://github.com/spotify/annoy/) to perform nearest neighbor search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we need to build an object that will let us perform nearest-neighbor search!\n",
    "from annoy import AnnoyIndex\n",
    "searcher = AnnoyIndex(3) # we need to give it the dimension of the representation\n",
    "# now, we need to insert each image...\n",
    "for idx in range(n_images):\n",
    "    searcher.add_item(idx, all_means[idx,:])\n",
    "searcher.build(10) # don't worry about the 10 -- it's an internal parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can search for nearest neighbors for images of different indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neighbors_of_first_image = searcher.get_nns_by_item(0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(neighbors_of_first_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_idx = 22\n",
    "neighbors_of_first_image = searcher.get_nns_by_item(image_idx, 10)\n",
    "print(\"Start image:\")\n",
    "fname = base_path + str(image_idx) + \".jpg\"\n",
    "image = Image.open(fname)\n",
    "display(image)\n",
    "r, g, b = compute_mean_color(image)\n",
    "display(constant_color_image(r,g,b))\n",
    "\n",
    "fname = base_path + str(idx) + \".jpg\"\n",
    "for idx in neighbors_of_first_image:\n",
    "    fname = base_path + str(idx) + \".jpg\"\n",
    "    image = Image.open(fname)\n",
    "    display(image)\n",
    "    r, g, b = compute_mean_color(image)\n",
    "    display(constant_color_image(r,g,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Using a Deep Neural Network to Represent Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the neural network library needs an in-order list of all files\n",
    "n_images = 5000\n",
    "base = 'coco_workshop/'\n",
    "all_files = []\n",
    "for idx in range(n_images):\n",
    "    all_files.append(base + str(idx) + '.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of efficiency, we will be running MobileNet, which is a roughly 13-layer neural network designed for mobile applications. In practice, neural networks can be up to 1000 layers deep!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import load_images_for_neural_network\n",
    "\n",
    "from keras.applications.mobilenet import MobileNet\n",
    "\n",
    "neural_net = MobileNet(include_top=False,\n",
    "                       input_shape=(224,224,3),\n",
    "                       pooling='avg')\n",
    "neural_net.summary()\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "images_for_nn = load_images_for_neural_network(all_files,\n",
    "                                               batch_size=64)\n",
    "\n",
    "representations = neural_net.predict_generator(images_for_nn,\n",
    "                                               steps=n_images/batch_size+1,\n",
    "                                               verbose=1)\n",
    "representations = representations[:5000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(representations.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "searcher = AnnoyIndex(1024) # we need to give it the dimension of the representation\n",
    "# now, we need to insert each image...\n",
    "for idx in range(n_images):\n",
    "    searcher.add_item(idx, representations[idx,:])\n",
    "searcher.build(10) # don't worry about the 10 -- it's an internal parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_idx = 119\n",
    "neighbors_of_image, dists = searcher.get_nns_by_item(image_idx, 10,include_distances=True)\n",
    "print(\"Start image:\")\n",
    "fname = base_path + str(image_idx) + \".jpg\"\n",
    "image = Image.open(fname)\n",
    "display(image)\n",
    "\n",
    "fname = base_path + str(idx) + \".jpg\"\n",
    "for dist_idx, idx in enumerate(neighbors_of_image[1:]):\n",
    "    fname = base_path + str(idx) + \".jpg\"\n",
    "    image = Image.open(fname)\n",
    "    display(image)\n",
    "    print(\"Distance to original image = \" + str(dists[dist_idx+1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
